{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Package installations\n",
    "Uncomment if use GPU rent services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install torch==1.9.0+cu111 torchvision==0.10.0+cu111 torchaudio===0.9.0 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nuscenes-devkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fix error : libGL.so.1: cannot open shared object file: No such file or directory\n",
    "# !apt install -y libgl1-mesa-glx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Running on  cuda:0\n",
      "Device count: 1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Running on ', device)\n",
    "print(f\"Device count: {torch.cuda.device_count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "CURRENT_PATH = f'{os.getcwd()}/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unzip nuScenes dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DatasetName = 'Dataset.zip'\n",
    "#\n",
    "# print(f'{CURRENT_PATH}{DatasetName}')\n",
    "#\n",
    "# with zipfile.ZipFile(DatasetName, 'r') as zip_ref:\n",
    "#     zip_ref.extractall(f'{CURRENT_PATH}{DatasetName}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NuScenes initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nuscenes import NuScenes\n",
    "from nuscenes.prediction import PredictHelper\n",
    "from nuscenes.eval.prediction.splits import get_prediction_challenge_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# This is the path where you stored your copy of the nuScenes dataset.\n",
    "DATAROOT = 'Dataset/'\n",
    "\n",
    "history_length = 2\n",
    "prediction_length = 6\n",
    "\n",
    "# Use v1.0-trainval or v1.0-mini\n",
    "nusc = NuScenes('v1.0-trainval', dataroot=DATAROOT, verbose=False)\n",
    "helper = PredictHelper(nusc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train len: 32186\n",
      "Val len: 8560\n",
      "Test len: 9041\n"
     ]
    }
   ],
   "source": [
    "train = get_prediction_challenge_split(\"train\", dataroot=DATAROOT)\n",
    "validation = get_prediction_challenge_split(\"train_val\", dataroot=DATAROOT)\n",
    "test = get_prediction_challenge_split(\"val\", dataroot=DATAROOT)\n",
    "\n",
    "print(f\"Train len: {len(train)}\\nVal len: {len(validation)}\\nTest len: {len(test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[:12000]\n",
    "validation = validation[:5000]\n",
    "test = test[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "PATH_TO_EPSILON_8_SET = f\"{DATAROOT}prediction_trajectory_sets/epsilon_8.pkl\"\n",
    "trajectories_set_8 = pickle.load(open(PATH_TO_EPSILON_8_SET, 'rb'))\n",
    "trajectories_set_8 = torch.Tensor(trajectories_set_8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Init datasets and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from nuscenes.prediction.input_representation.static_layers import StaticLayerRasterizer\n",
    "\n",
    "import numpy as np\n",
    "from typing import List\n",
    "\n",
    "class NuscenesDataset(Dataset):\n",
    "    def __init__(self, tokens: List[str], helper: PredictHelper):\n",
    "        self.tokens = tokens\n",
    "        self.static_layer_representation = StaticLayerRasterizer(helper)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens)\n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "\n",
    "        token = self.tokens[index]\n",
    "        instance_token, sample_token = token.split(\"_\")\n",
    "\n",
    "        image = self.static_layer_representation.make_representation(instance_token, sample_token)\n",
    "        image = torch.Tensor(image).permute(2, 0, 1)\n",
    "\n",
    "        # NaN Values processing\n",
    "        def agent_param_processing(value):\n",
    "            if np.isnan(value):\n",
    "                return -1\n",
    "            return value\n",
    "        \n",
    "        vel = helper.get_velocity_for_agent(instance_token, sample_token)\n",
    "        vel = agent_param_processing(vel)\n",
    "        \n",
    "        accel = helper.get_acceleration_for_agent(instance_token, sample_token)\n",
    "        accel = agent_param_processing(accel)\n",
    "        \n",
    "        heading_cr = helper.get_heading_change_rate_for_agent(instance_token, sample_token)\n",
    "        heading_cr = agent_param_processing(heading_cr)\n",
    "                \n",
    "        agent_state_vector = torch.Tensor([vel, accel, heading_cr])\n",
    "\n",
    "        ground_truth = helper.get_future_for_agent(instance_token, sample_token, prediction_length, in_agent_frame=True)\n",
    "\n",
    "        # Convert to [batch_size, 1, 12, 2]\n",
    "        # Because loss function need that format\n",
    "        ground_truth = np.expand_dims(ground_truth, 0)\n",
    "\n",
    "        return image, agent_state_vector, ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_ds = NuscenesDataset(train, helper)\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "train_val_ds = NuscenesDataset(validation, helper)\n",
    "train_val_dl = DataLoader(train_ds, batch_size=batch_size * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 500, 500])\n",
      "torch.Size([32, 3])\n",
      "torch.Size([32, 1, 12, 2])\n",
      "Preprocessing states:\n",
      "tensor([[ 3.8829e+00,  4.2858e+00,  3.4914e-02],\n",
      "        [ 7.4828e+00,  9.7190e-01,  0.0000e+00],\n",
      "        [ 2.6254e+00,  1.1437e-01,  9.6029e-02],\n",
      "        [ 5.5119e+00,  7.8166e-01,  3.1396e-01],\n",
      "        [ 1.7754e+00,  2.1683e-01, -1.2660e-01],\n",
      "        [ 4.9390e+00,  2.2711e-03, -3.4954e-02],\n",
      "        [ 6.4788e-01, -2.9224e-04,  0.0000e+00],\n",
      "        [ 1.3208e+00,  1.7265e+00, -4.1853e-02],\n",
      "        [ 1.0706e+01, -2.5083e-03,  1.7458e-02],\n",
      "        [ 1.2010e+01,  1.1310e+00,  8.8835e-16],\n",
      "        [ 5.2919e+00, -1.8864e+00,  3.3168e-01],\n",
      "        [ 9.6307e+00, -4.3436e-01,  0.0000e+00],\n",
      "        [ 1.0469e+01, -1.0000e+00,  8.7288e-03],\n",
      "        [ 5.1837e+00, -3.1336e-01, -5.0745e-01],\n",
      "        [ 9.7165e-01, -2.1897e+00,  0.0000e+00],\n",
      "        [ 8.9414e+00, -1.0000e+00,  0.0000e+00],\n",
      "        [ 9.0079e+00, -5.9894e-03,  0.0000e+00],\n",
      "        [ 1.9260e+00,  1.9186e+00, -1.0463e-02],\n",
      "        [ 1.6071e+00,  1.9847e-04,  0.0000e+00],\n",
      "        [ 1.1468e+01, -3.7591e-02,  0.0000e+00],\n",
      "        [ 1.0496e+01,  5.1408e-04,  0.0000e+00],\n",
      "        [ 3.2098e+00, -7.6509e-01, -4.8983e-01],\n",
      "        [ 8.8263e-01, -1.9217e-01, -6.9830e-02],\n",
      "        [ 6.5537e+00, -4.9473e+00, -2.3317e-02],\n",
      "        [ 5.6744e+00, -6.5802e-03, -3.4913e-02],\n",
      "        [ 9.4840e+00, -3.5710e-03,  3.3251e-02],\n",
      "        [ 6.3178e+00,  1.1400e-02,  5.2372e-02],\n",
      "        [ 8.0353e+00, -1.7596e-03,  0.0000e+00],\n",
      "        [ 1.2223e+00,  1.8329e-01,  3.8908e-02],\n",
      "        [ 6.7980e+00,  8.2123e-04, -2.9085e-02],\n",
      "        [ 1.1345e+01,  2.3408e-03,  0.0000e+00],\n",
      "        [ 8.0727e+00,  1.0155e+00, -1.0438e-02]])\n"
     ]
    }
   ],
   "source": [
    "image, state, ground_truth = next(iter(train_dl))\n",
    "print(image.size())\n",
    "print(state.size())\n",
    "print(ground_truth.size())\n",
    "\n",
    "print(\"Preprocessing states:\")\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Init ML prediction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from nuscenes.prediction.models.backbone import ResNetBackbone\n",
    "import torchvision.models as models\n",
    "\n",
    "# Torchvision backbone\n",
    "backbone = models.resnext50_32x4d(pretrained=True)\n",
    "\n",
    "# Build-in backbone\n",
    "#backbone = ResNetBackbone('resnet50')\n",
    "\n",
    "# Set backbone to non-trainable\n",
    "def set_parameter_requires_grad(model):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "set_parameter_requires_grad(backbone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\impor\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import SGD\n",
    "from nuscenes.prediction.models.covernet import CoverNet, ConstantLatticeLoss\n",
    "\n",
    "NUM_MODES = 64\n",
    "\n",
    "model = CoverNet(backbone, num_modes=NUM_MODES)\n",
    "model = model.to(device)\n",
    "\n",
    "loss_function = ConstantLatticeLoss(trajectories_set_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t head.0.weight\n",
      "\t head.0.bias\n",
      "\t head.1.weight\n",
      "\t head.1.bias\n"
     ]
    }
   ],
   "source": [
    "# Pass to optimizer only params with requires_grad\n",
    "params_to_update = []\n",
    "\n",
    "for name,param in model.named_parameters():\n",
    "    if param.requires_grad == True:\n",
    "        params_to_update.append(param)\n",
    "        print(\"\\t\",name)\n",
    "\n",
    "optimizer = SGD(params_to_update, lr=5e-4, momentum=0.9, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import copy\n",
    "import time\n",
    "\n",
    "def loss_batch(model, loss_func, img, state_vec, ground_truth, opt=None):\n",
    "    img = img.to(device)\n",
    "    state_vec = state_vec.to(device)\n",
    "    ground_truth = ground_truth.to(device)\n",
    "    \n",
    "    predicted_logits = model(img, state_vec)\n",
    "    loss = loss_func(predicted_logits, ground_truth)\n",
    "\n",
    "    # For validation optimizer is None, thus we dont perform backprop\n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    # Return losses and amount of items\n",
    "    # print(f\"{loss.item()}; {len(img)}\")\n",
    "    return loss.item(), len(img)\n",
    "\n",
    "\n",
    "def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
    "    best_loss = 999.0\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        start_epoch_time = time.time()\n",
    "        print(f'Epoch: {epoch + 1}/{epochs}')\n",
    "        print('-' * 10)\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "        for img, state_vec, gt in tqdm(train_dl):\n",
    "            loss_batch(model, loss_func, img, state_vec, gt, opt)\n",
    "\n",
    "        model.eval()\n",
    "        print(\"Validation step\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # TODO: Using tqdm\n",
    "            losses, nums = zip(\n",
    "                *[loss_batch(model, loss_func, img, state_vec, gt) for img, state_vec, gt in valid_dl]\n",
    "            )\n",
    "\n",
    "        val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
    "        \n",
    "        # deep copy the model\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}; Loss: {val_loss:0.2f}; Best: {best_loss:0.2f} Time: {(time.time() - start_epoch_time):0.2f} sec;\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 3  # how many epochs to train for\n",
    "# fit(epochs, model, loss_function, optimizer, train_dl, train_val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), '/root/model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "CoverNet(\n  (backbone): ResNet(\n    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (relu): ReLU(inplace=True)\n    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n    (layer1): Sequential(\n      (0): Bottleneck(\n        (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (downsample): Sequential(\n          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): Bottleneck(\n        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (2): Bottleneck(\n        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n    )\n    (layer2): Sequential(\n      (0): Bottleneck(\n        (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (downsample): Sequential(\n          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): Bottleneck(\n        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (2): Bottleneck(\n        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (3): Bottleneck(\n        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n    )\n    (layer3): Sequential(\n      (0): Bottleneck(\n        (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (downsample): Sequential(\n          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): Bottleneck(\n        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (2): Bottleneck(\n        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (3): Bottleneck(\n        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (4): Bottleneck(\n        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (5): Bottleneck(\n        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n    )\n    (layer4): Sequential(\n      (0): Bottleneck(\n        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (downsample): Sequential(\n          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): Bottleneck(\n        (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (2): Bottleneck(\n        (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n    )\n    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n    (fc): Linear(in_features=2048, out_features=1000, bias=True)\n  )\n  (head): ModuleList(\n    (0): Linear(in_features=1003, out_features=4096, bias=True)\n    (1): Linear(in_features=4096, out_features=64, bias=True)\n  )\n)"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('./Models/model_data_20k_e25_loss_1-50.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "NPY_DATA_PATH = './NpyDataset/'\n",
    "\n",
    "test_features = np.load(f'{NPY_DATA_PATH}data_test_features_5k.npy')\n",
    "test_states = np.load(f'{NPY_DATA_PATH}data_test_states_5k.npy')\n",
    "test_labels = np.load(f'{NPY_DATA_PATH}data_test_labels_5k.npy')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "def PlotPrediction(future, predict):\n",
    "  plt.figure(figsize=(6, 6))\n",
    "\n",
    "  plt.scatter(future[:, 1], -future[:, 0], c='orange', s=10)\n",
    "  plt.scatter(predict[:, 1], -predict[:, 0], c='g', s=10)\n",
    "\n",
    "  # Keep aspect ratio of axis\n",
    "  plt.axis('equal')\n",
    "  plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "{'MinFDEK': array([[0.],\n        [0.],\n        [0.],\n        ...,\n        [0.],\n        [0.],\n        [0.]]),\n 'MinADEK': array([[0., 0.],\n        [0., 0.],\n        [0., 0.],\n        ...,\n        [0., 0.],\n        [0., 0.],\n        [0., 0.]]),\n 'MissRateTopK_2': array([[0., 0.],\n        [0., 0.],\n        [0., 0.],\n        ...,\n        [0., 0.],\n        [0., 0.],\n        [0., 0.]])}"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nuscenes.eval.prediction.metrics as metrics\n",
    "\n",
    "trajectories_set_8_np = trajectories_set_8.numpy()\n",
    "\n",
    "metric_functions = [metrics.MinFDEK([1], aggregators=[metrics.RowMean()]),\n",
    "                    metrics.MinADEK([5, 10], aggregators=[metrics.RowMean()]),\n",
    "                    metrics.MissRateTopK([5, 10], tolerance=2, aggregators=[metrics.RowMean()])]\n",
    "\n",
    "num_predictions = len(test_labels) # Amount of prediction rows\n",
    "\n",
    "metrics_container = {metric.name: np.zeros((num_predictions, metric.shape)) for metric in metric_functions}\n",
    "metrics_container"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Clear version of metric calculation\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [03:35<00:00, 23.21it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for idx, x in enumerate(tqdm(test_labels)):\n",
    "#for idx in range(len(test_labels)):\n",
    "    # Make prediction\n",
    "    img = torch.Tensor(test_features[idx].reshape((500, 500, 3))).permute(2, 0, 1).unsqueeze(0)\n",
    "    img = img.to(device)\n",
    "\n",
    "    state = torch.Tensor(np.array([test_states[idx]])).to(device)\n",
    "    state = state.to(device)\n",
    "\n",
    "    logits = model(img, state)\n",
    "    mode_probabilities = np.array([logits.cpu().detach().numpy()[0]])[0]\n",
    "\n",
    "    # Create prediction object\n",
    "    instance_tkn, sample_tkn = test[idx].split(\"_\")\n",
    "    prediction = metrics.Prediction(instance_tkn, sample_tkn, trajectories_set_8_np, mode_probabilities)\n",
    "\n",
    "    # Get ground_truth\n",
    "    gt = test_labels[idx].reshape((12, 2))\n",
    "\n",
    "    # Calculate metrics\n",
    "    for metric in metric_functions:\n",
    "        metrics_container[metric.name][idx] = metric(gt, prediction)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "defaultdict(dict,\n            {'MinFDEK': {'RowMean': [12.143089777763683]},\n             'MinADEK': {'RowMean': [2.5416744372181608, 2.20823303355608]},\n             'MissRateTopK_2': {'RowMean': [0.9212, 0.9182]}})"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "aggregations: Dict[str, Dict[str, List[float]]] = defaultdict(dict)\n",
    "\n",
    "for metric in metric_functions:\n",
    "    for agg in metric.aggregators:\n",
    "        aggregations[metric.name][agg.name] = agg(metrics_container[metric.name])\n",
    "\n",
    "aggregations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "categories = []\n",
    "\n",
    "for sample in train:\n",
    "    instance_tkn, sample_tkn =  sample.split(\"_\")\n",
    "    instance_category_token = nusc.get('instance', instance_tkn)['category_token']\n",
    "\n",
    "    category_name = nusc.get('category', instance_category_token)['name']\n",
    "\n",
    "    if category_name not in categories:\n",
    "        categories.append(category_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "['vehicle.bus.rigid',\n 'vehicle.car',\n 'vehicle.truck',\n 'vehicle.construction',\n 'vehicle.bus.bendy',\n 'vehicle.emergency.police']"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "categories"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}