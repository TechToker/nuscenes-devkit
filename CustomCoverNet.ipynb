{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Package installations\n",
    "Uncomment if use GPU rent services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install torch==1.9.0+cu111 torchvision==0.10.0+cu111 torchaudio===0.9.0 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nuscenes-devkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fix error : libGL.so.1: cannot open shared object file: No such file or directory\n",
    "# !apt install -y libgl1-mesa-glx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Running on  cuda:0\n",
      "Device count: 1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Running on ', device)\n",
    "print(f\"Device count: {torch.cuda.device_count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "CURRENT_PATH = f'{os.getcwd()}/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unzip nuScenes dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DatasetName = 'Dataset.zip'\n",
    "#\n",
    "# print(f'{CURRENT_PATH}{DatasetName}')\n",
    "#\n",
    "# with zipfile.ZipFile(DatasetName, 'r') as zip_ref:\n",
    "#     zip_ref.extractall(f'{CURRENT_PATH}{DatasetName}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NuScenes initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nuscenes import NuScenes\n",
    "from nuscenes.prediction import PredictHelper\n",
    "from nuscenes.eval.prediction.splits import get_prediction_challenge_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# This is the path where you stored your copy of the nuScenes dataset.\n",
    "DATAROOT = 'Dataset/'\n",
    "\n",
    "history_length = 2\n",
    "prediction_length = 6\n",
    "\n",
    "# Use v1.0-trainval or v1.0-mini\n",
    "nusc = NuScenes('v1.0-trainval', dataroot=DATAROOT, verbose=False)\n",
    "helper = PredictHelper(nusc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train len: 32186\n",
      "Val len: 8560\n",
      "Test len: 9041\n"
     ]
    }
   ],
   "source": [
    "train = get_prediction_challenge_split(\"train\", dataroot=DATAROOT)\n",
    "validation = get_prediction_challenge_split(\"train_val\", dataroot=DATAROOT)\n",
    "test = get_prediction_challenge_split(\"val\", dataroot=DATAROOT)\n",
    "\n",
    "print(f\"Train len: {len(train)}\\nVal len: {len(validation)}\\nTest len: {len(test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[:12000]\n",
    "validation = validation[:5000]\n",
    "test = test[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "PATH_TO_EPSILON_8_SET = f\"{DATAROOT}prediction_trajectory_sets/epsilon_8.pkl\"\n",
    "trajectories_set_8 = pickle.load(open(PATH_TO_EPSILON_8_SET, 'rb'))\n",
    "trajectories_set_8 = torch.Tensor(trajectories_set_8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Init datasets and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from nuscenes.prediction.input_representation.static_layers import StaticLayerRasterizer\n",
    "\n",
    "import numpy as np\n",
    "from typing import List\n",
    "\n",
    "class NuscenesDataset(Dataset):\n",
    "    def __init__(self, tokens: List[str], helper: PredictHelper):\n",
    "        self.tokens = tokens\n",
    "        self.static_layer_representation = StaticLayerRasterizer(helper)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens)\n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "\n",
    "        token = self.tokens[index]\n",
    "        instance_token, sample_token = token.split(\"_\")\n",
    "\n",
    "        image = self.static_layer_representation.make_representation(instance_token, sample_token)\n",
    "        image = torch.Tensor(image).permute(2, 0, 1)\n",
    "\n",
    "        # NaN Values processing\n",
    "        def agent_param_processing(value):\n",
    "            if np.isnan(value):\n",
    "                return -1\n",
    "            return value\n",
    "        \n",
    "        vel = helper.get_velocity_for_agent(instance_token, sample_token)\n",
    "        vel = agent_param_processing(vel)\n",
    "        \n",
    "        accel = helper.get_acceleration_for_agent(instance_token, sample_token)\n",
    "        accel = agent_param_processing(accel)\n",
    "        \n",
    "        heading_cr = helper.get_heading_change_rate_for_agent(instance_token, sample_token)\n",
    "        heading_cr = agent_param_processing(heading_cr)\n",
    "                \n",
    "        agent_state_vector = torch.Tensor([vel, accel, heading_cr])\n",
    "\n",
    "        ground_truth = helper.get_future_for_agent(instance_token, sample_token, prediction_length, in_agent_frame=True)\n",
    "\n",
    "        # Convert to [batch_size, 1, 12, 2]\n",
    "        # Because loss function need that format\n",
    "        ground_truth = np.expand_dims(ground_truth, 0)\n",
    "\n",
    "        return image, agent_state_vector, ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_ds = NuscenesDataset(train, helper)\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "train_val_ds = NuscenesDataset(validation, helper)\n",
    "train_val_dl = DataLoader(train_ds, batch_size=batch_size * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 500, 500])\n",
      "torch.Size([32, 3])\n",
      "torch.Size([32, 1, 12, 2])\n",
      "Preprocessing states:\n",
      "tensor([[ 6.5560e+00, -7.0714e-01,  7.7550e-02],\n",
      "        [ 1.0361e+01,  1.4954e-04,  0.0000e+00],\n",
      "        [ 3.8700e+00, -1.0000e+00, -1.9387e-01],\n",
      "        [ 3.8800e+00,  2.4926e-01,  0.0000e+00],\n",
      "        [ 1.8056e-01,  6.5437e-04,  1.3966e-02],\n",
      "        [ 1.1278e+01,  2.4033e-03,  0.0000e+00],\n",
      "        [ 7.4143e+00, -1.0000e+00,  1.8384e-02],\n",
      "        [ 5.7498e+00, -5.7486e-01,  0.0000e+00],\n",
      "        [ 9.9595e+00, -6.1523e-01,  7.7553e-02],\n",
      "        [ 2.1336e+00, -4.8064e-02, -3.0270e-01],\n",
      "        [ 5.0137e-01, -5.0147e-03,  0.0000e+00],\n",
      "        [ 9.2956e+00, -1.6963e+00,  0.0000e+00],\n",
      "        [ 1.6924e+00, -3.6643e-04,  1.6621e-02],\n",
      "        [ 3.3864e+00,  1.2458e+00,  4.2652e-01],\n",
      "        [ 2.4337e-02,  3.0541e-03,  2.1503e-02],\n",
      "        [ 3.7095e+00, -9.4885e-04,  2.6958e-01],\n",
      "        [ 3.3353e+00,  3.8862e-03,  2.5437e-01],\n",
      "        [ 7.2877e+00,  5.9953e+00, -2.9668e-01],\n",
      "        [ 1.2844e+00,  1.9858e+00,  0.0000e+00],\n",
      "        [ 5.6007e+00, -1.2828e+00, -1.7438e-02],\n",
      "        [ 6.3026e+00, -2.4620e-03, -4.3596e-03],\n",
      "        [ 1.7191e-02, -1.3610e-01,  0.0000e+00],\n",
      "        [ 3.0981e+00, -9.8587e-01,  3.4873e-02],\n",
      "        [ 6.1120e+00,  2.7038e-04,  0.0000e+00],\n",
      "        [ 3.5410e+00, -5.9400e-03,  0.0000e+00],\n",
      "        [ 2.1290e+00,  3.4495e-01, -1.2678e-01],\n",
      "        [ 1.0529e+01, -2.8123e-01,  0.0000e+00],\n",
      "        [ 4.3654e+00, -1.4261e-02,  0.0000e+00],\n",
      "        [ 3.2472e+00, -1.3582e+00,  6.3445e-02],\n",
      "        [ 4.1963e+00,  1.7615e+00, -5.1306e-03],\n",
      "        [ 2.6810e-01,  6.5942e-04, -1.7458e-02],\n",
      "        [ 3.7523e+00,  5.6335e-01,  0.0000e+00]])\n"
     ]
    }
   ],
   "source": [
    "image, state, ground_truth = next(iter(train_dl))\n",
    "print(image.size())\n",
    "print(state.size())\n",
    "print(ground_truth.size())\n",
    "\n",
    "print(\"Preprocessing states:\")\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Init ML prediction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from nuscenes.prediction.models.backbone import ResNetBackbone\n",
    "import torchvision.models as models\n",
    "\n",
    "# Torchvision backbone\n",
    "backbone = models.resnext50_32x4d(pretrained=True)\n",
    "\n",
    "# Build-in backbone\n",
    "#backbone = ResNetBackbone('resnet50')\n",
    "\n",
    "# Set backbone to non-trainable\n",
    "def set_parameter_requires_grad(model):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "set_parameter_requires_grad(backbone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\impor\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import SGD\n",
    "from nuscenes.prediction.models.covernet import CoverNet, ConstantLatticeLoss\n",
    "\n",
    "NUM_MODES = 64\n",
    "\n",
    "model = CoverNet(backbone, num_modes=NUM_MODES)\n",
    "model = model.to(device)\n",
    "\n",
    "loss_function = ConstantLatticeLoss(trajectories_set_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t head.0.weight\n",
      "\t head.0.bias\n",
      "\t head.1.weight\n",
      "\t head.1.bias\n"
     ]
    }
   ],
   "source": [
    "# Pass to optimizer only params with requires_grad\n",
    "params_to_update = []\n",
    "\n",
    "for name,param in model.named_parameters():\n",
    "    if param.requires_grad == True:\n",
    "        params_to_update.append(param)\n",
    "        print(\"\\t\",name)\n",
    "\n",
    "optimizer = SGD(params_to_update, lr=5e-4, momentum=0.9, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import copy\n",
    "import time\n",
    "\n",
    "def loss_batch(model, loss_func, img, state_vec, ground_truth, opt=None):\n",
    "    img = img.to(device)\n",
    "    state_vec = state_vec.to(device)\n",
    "    ground_truth = ground_truth.to(device)\n",
    "    \n",
    "    predicted_logits = model(img, state_vec)\n",
    "    loss = loss_func(predicted_logits, ground_truth)\n",
    "\n",
    "    # For validation optimizer is None, thus we dont perform backprop\n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    # Return losses and amount of items\n",
    "    # print(f\"{loss.item()}; {len(img)}\")\n",
    "    return loss.item(), len(img)\n",
    "\n",
    "\n",
    "def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
    "    best_loss = 999.0\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        start_epoch_time = time.time()\n",
    "        print(f'Epoch: {epoch + 1}/{epochs}')\n",
    "        print('-' * 10)\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "        for img, state_vec, gt in tqdm(train_dl):\n",
    "            loss_batch(model, loss_func, img, state_vec, gt, opt)\n",
    "\n",
    "        model.eval()\n",
    "        print(\"Validation step\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # TODO: Using tqdm\n",
    "            losses, nums = zip(\n",
    "                *[loss_batch(model, loss_func, img, state_vec, gt) for img, state_vec, gt in valid_dl]\n",
    "            )\n",
    "\n",
    "        val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
    "        \n",
    "        # deep copy the model\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}; Loss: {val_loss:0.2f}; Best: {best_loss:0.2f} Time: {(time.time() - start_epoch_time):0.2f} sec;\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 3  # how many epochs to train for\n",
    "# fit(epochs, model, loss_function, optimizer, train_dl, train_val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), '/root/model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "CoverNet(\n  (backbone): ResNet(\n    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (relu): ReLU(inplace=True)\n    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n    (layer1): Sequential(\n      (0): Bottleneck(\n        (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (downsample): Sequential(\n          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): Bottleneck(\n        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (2): Bottleneck(\n        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n    )\n    (layer2): Sequential(\n      (0): Bottleneck(\n        (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (downsample): Sequential(\n          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): Bottleneck(\n        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (2): Bottleneck(\n        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (3): Bottleneck(\n        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n    )\n    (layer3): Sequential(\n      (0): Bottleneck(\n        (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (downsample): Sequential(\n          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): Bottleneck(\n        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (2): Bottleneck(\n        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (3): Bottleneck(\n        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (4): Bottleneck(\n        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (5): Bottleneck(\n        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n    )\n    (layer4): Sequential(\n      (0): Bottleneck(\n        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (downsample): Sequential(\n          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): Bottleneck(\n        (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (2): Bottleneck(\n        (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n    )\n    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n    (fc): Linear(in_features=2048, out_features=1000, bias=True)\n  )\n  (head): ModuleList(\n    (0): Linear(in_features=1003, out_features=4096, bias=True)\n    (1): Linear(in_features=4096, out_features=64, bias=True)\n  )\n)"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('./Models/model_data_5000_e25_loss_1-15.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "NPY_DATA_PATH = './NpyDataset/'\n",
    "\n",
    "test_features = np.load(f'{NPY_DATA_PATH}data_test_features_5k.npy')\n",
    "test_states = np.load(f'{NPY_DATA_PATH}data_test_states_5k.npy')\n",
    "test_labels = np.load(f'{NPY_DATA_PATH}data_test_labels_5k.npy')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "def PlotPrediction(future, predict):\n",
    "  plt.figure(figsize=(6, 6))\n",
    "\n",
    "  plt.scatter(future[:, 1], -future[:, 0], c='orange', s=10)\n",
    "  plt.scatter(predict[:, 1], -predict[:, 0], c='g', s=10)\n",
    "\n",
    "  # Keep aspect ratio of axis\n",
    "  plt.axis('equal')\n",
    "  plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.35029012 4.35029012]]\n",
      "[[3.67147222 3.67147222]]\n",
      "[[3.76456152 3.02057887]]\n",
      "[[3.24151024 2.26132469]]\n",
      "[[3.72568621 2.34562331]]\n"
     ]
    }
   ],
   "source": [
    "import nuscenes.eval.prediction.metrics as metrics\n",
    "\n",
    "trajectories_set_8_np = trajectories_set_8.numpy()\n",
    "\n",
    "minFDE_k = metrics.MinFDEK([1, 5], aggregators=[metrics.RowMean()])\n",
    "\n",
    "metrics_container = []\n",
    "\n",
    "for idx in range(5):\n",
    "    # Make prediction\n",
    "    img = torch.Tensor(test_features[idx].reshape((500, 500, 3))).permute(2, 0, 1).unsqueeze(0)\n",
    "    img = img.to(device)\n",
    "\n",
    "    state = torch.Tensor(np.array([test_states[idx]])).to(device)\n",
    "    state = state.to(device)\n",
    "\n",
    "    logits = model(img, state)\n",
    "    mode_probabilities = np.array([logits.cpu().detach().numpy()[0]])[0]\n",
    "\n",
    "    # Create prediction object\n",
    "    instance_tkn, sample_tkn = test[idx].split(\"_\")\n",
    "    prediction = metrics.Prediction(instance_tkn, sample_tkn, trajectories_set_8_np, mode_probabilities)\n",
    "\n",
    "    # Get ground_truth\n",
    "    gt = test_labels[idx].reshape((12, 2))\n",
    "\n",
    "    # Calculate metrics\n",
    "    fde = minFDE_k(gt, prediction)\n",
    "    metrics_container.append(fde)\n",
    "    print(fde)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Clear version of metric calculation\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[4.35029012, 4.35029012]]), array([[3.67147222, 3.67147222]]), array([[3.76456152, 3.02057887]]), array([[3.24151024, 2.26132469]]), array([[3.72568621, 2.34562331]])]\n",
      "[[3.7507040602032733, 3.1298578419185428]]\n"
     ]
    }
   ],
   "source": [
    "print(metrics_container)\n",
    "\n",
    "for agg in minFDE_k.aggregators:\n",
    "    a = agg(np.array(metrics_container))\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]: mFDE: 4.350290120077532; mADE: 3.550774972838594\n",
      "[1]: mFDE: 3.6714722223046516; mADE: 3.337161391862899\n",
      "[2]: mFDE: 3.764561515284531; mADE: 3.055059685830171\n",
      "[3]: mFDE: 3.2415102377064393; mADE: 2.7349237477942556\n",
      "[4]: mFDE: 3.7256862056432123; mADE: 3.062043088546279\n"
     ]
    }
   ],
   "source": [
    "stacked_trajs = trajectories_set_8.numpy()\n",
    "\n",
    "min_ade_k = 5\n",
    "\n",
    "total_min_fde = 0\n",
    "total_min_ade = 0\n",
    "\n",
    "#MissRateTopK = metrics.MissRateTopK([5, 10], tolerance=2, aggregators=[metrics.RowMean()])\n",
    "\n",
    "#for idx in range(len(test_labels)):\n",
    "for idx in range(5):\n",
    "    img = torch.Tensor(test_features[idx].reshape((500, 500, 3))).permute(2, 0, 1).unsqueeze(0)\n",
    "    img = img.to(device)\n",
    "\n",
    "    state = torch.Tensor(np.array([test_states[idx]])).to(device)\n",
    "    state = state.to(device)\n",
    "\n",
    "    # print(img.size())\n",
    "    # print(state.size())\n",
    "\n",
    "    logits = model(img, state)\n",
    "    mode_probabilities = np.array([logits.cpu().detach().numpy()[0]])\n",
    "\n",
    "    # TODO: Use metrics.stack_ground_truth\n",
    "    N = 64\n",
    "    gt = test_labels[idx].reshape((12, 2))\n",
    "    stacked_ground_truth = np.empty((N, 12, 2))\n",
    "    for i in range(N):\n",
    "        stacked_ground_truth[i] = gt\n",
    "\n",
    "    #min_fde_k = 1\n",
    "    min_fde = metrics.min_fde_k(stacked_trajs, stacked_ground_truth, mode_probabilities)\n",
    "    min_ade = metrics.min_ade_k(stacked_trajs, stacked_ground_truth, mode_probabilities)\n",
    "    miss_rate = metrics.MissRateTopK\n",
    "\n",
    "    min_fde = min_fde[0][0]\n",
    "    min_ade = min(min_ade[0][:min_ade_k])\n",
    "\n",
    "    # Plot ground_truth and Top1 trajectory\n",
    "    # sorted_logits_indexes = logits.argsort(descending=True)\n",
    "    # sorted_trajectories = trajectories_set_8[sorted_logits_indexes][0]\n",
    "\n",
    "    # for i in range(min_ade_k):\n",
    "    #     PlotPrediction(gt, sorted_trajectories[i])\n",
    "\n",
    "    print(f\"[{idx}]: mFDE: {min_fde}; mADE: {min_ade}\")\n",
    "    total_min_fde += min_fde\n",
    "    total_min_ade += min_ade\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "(3.7507040602032733, 3.1479925773744393)"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 5 # len(test_labels)\n",
    "\n",
    "total_min_fde / n, total_min_ade / n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "instance_token, sample_token = test[0].split('_')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 5.83557606e+00,  3.56018233e+00, -4.36437654e+00,\n         4.93365765e+00, -1.11988049e+01, -1.22068195e+01,\n         5.44546318e+00,  3.47506762e+00,  2.52648759e+00,\n         6.95708275e+00,  1.13715184e+00,  2.36022234e+00,\n        -7.45455742e+00,  5.44341707e+00,  2.45202971e+00,\n        -2.93625879e+00, -4.13234532e-03,  3.38360047e+00,\n        -1.07468967e+01, -8.43985844e+00,  8.25137734e-01,\n         1.98839319e+00,  5.07631826e+00,  2.12939382e+00,\n         5.12734509e+00,  3.80032206e+00,  2.09863448e+00,\n         8.32696795e-01, -4.28695679e+00, -1.23020220e+01,\n         1.78788447e+00,  1.03175819e+00, -5.53425407e+00,\n         2.09780192e+00, -5.24218607e+00, -3.53525591e+00,\n        -4.80795860e+00,  4.16410780e+00,  2.38868237e+00,\n        -2.16933298e+00,  9.68165934e-01, -3.20030141e+00,\n         4.50264883e+00,  2.16203046e+00, -2.34711123e+00,\n         3.72334504e+00,  3.80144089e-01,  3.40158916e+00,\n        -7.92281091e-01, -2.27562451e+00,  3.01750159e+00,\n        -6.32973719e+00,  2.71189642e+00, -5.04061556e+00,\n        -1.50396228e+00,  1.77471304e+00,  1.28772378e+00,\n         1.00158825e+01,  7.23196268e+00,  1.59146976e+00,\n        -1.62057662e+00,  3.52826267e-01, -2.54447651e+00,\n        -2.07497215e+00]], dtype=float32)"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mode_probabilities"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.35029012 4.35029012]]\n",
      "[[3.55077497 3.55077497]]\n",
      "[[ True  True]]\n"
     ]
    }
   ],
   "source": [
    "prediction = metrics.Prediction(instance_token, sample_token, trajectories_set_8.numpy(), mode_probabilities[0])\n",
    "\n",
    "minFDE_k = metrics.MinFDEK([1, 5], aggregators=[metrics.RowMean()])\n",
    "minADE_k = metrics.MinADEK([5, 10], aggregators=[metrics.RowMean()])\n",
    "missRate_k = metrics.MissRateTopK([5, 10], tolerance=2, aggregators=[metrics.RowMean()])\n",
    "\n",
    "fde = minFDE_k(gt, prediction)\n",
    "print(fde)\n",
    "\n",
    "ade = minADE_k(gt, prediction)\n",
    "print(ade)\n",
    "\n",
    "miss_rate = missRate_k(gt, prediction)\n",
    "print(miss_rate)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# import nuscenes.eval.prediction.metrics as metrics\n",
    "#\n",
    "# a = metrics.min_fde_k(stacked_trajs, stacked_ground_truth, mode_probabilities)\n",
    "# a\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# from nuscenes.eval.prediction.tests.test_metrics import TestFunctions\n",
    "\n",
    "# tf = TestFunctions()\n",
    "\n",
    "# tf.setUp()\n",
    "# tf.test_min_fde_k_many_batches_and_modes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}